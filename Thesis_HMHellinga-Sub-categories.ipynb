{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9666f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabelen met meer correlatie verwijderd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7202f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import classification_report, roc_auc_score, plot_confusion_matrix, make_scorer, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf4c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mhell\\AppData\\Local\\Temp\\ipykernel_4884\\1274915200.py:1: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_people_dataset = pd.read_csv('C:\\\\Users\\\\Mhell\\\\Downloads\\\\Thesis data\\\\people (1).csv.gz', encoding='ISO-8859-1')\n"
     ]
    }
   ],
   "source": [
    "df_people_dataset = pd.read_csv('C:\\\\Users\\\\Mhell\\\\Downloads\\\\Thesis data\\\\people (1).csv.gz', encoding='ISO-8859-1')\n",
    "df3_location_dataset = pd.read_csv('C:\\\\Users\\\\Mhell\\\\Downloads\\\\Thesis data\\\\locations.csv.gz', encoding='ISO-8859-1')\n",
    "df4_connect_dataset = pd.read_csv('C:\\\\Users\\\\Mhell\\\\Downloads\\\\Thesis data\\\\connect.csv.gz', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56de5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we filter the instances who don't represent an collaboration and are an open end in the tree (pid2 is 0)\n",
    "# we do this in df4: the 'connect' dataset containing collaborations between academici.\n",
    "filtered_df = df4_connect_dataset[df4_connect_dataset['pid2'] != 0]\n",
    "\n",
    "# Then we count the amount of chilren every PID1 has. We do this by counting how often they are in the PID1 column\n",
    "# since every collaboartion represents a different instance.\n",
    "frequentie = filtered_df['pid1'].value_counts()\n",
    "\n",
    "# We counted the amount of children (fecundity) and safe it in a new variable: 'fecundity'\n",
    "df4_connect_dataset['fecundity'] = df4_connect_dataset['pid1'].map(frequentie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec685c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the 'people' dataset, containing all the individual researchers (represented by a personal indintificator (pid))\n",
    "# we change pid to pid1 and connect the fecundity to this pid. After that, we add 'fecundity' to the connect dataset.\n",
    "df_people_dataset = df_people_dataset.rename(columns={'pid': 'pid1'})\n",
    "df_people_dataset = df_people_dataset.merge(df4_connect_dataset[['pid1', 'fecundity']], on='pid1', how='left')\n",
    "\n",
    "# we change the name of the column back to the original (in the people dataset) and delete duplicates.\n",
    "df_people_dataset = df_people_dataset.rename(columns={'pid1': 'pid'})\n",
    "df_people_dataset = df_people_dataset.drop_duplicates(subset='pid', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16cedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the unimportant columns in the connect dataset because we are using this dataset as the basis for the merged dataset\n",
    "df4_connect_dataset.drop(['addedby', 'dateadded'], axis=1, inplace=True)\n",
    "df_people_dataset.drop(['firstname', 'middlename', 'addedby','homepage', 'orcid_id', 's2id', 'dateadded','area','award','lastname'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03063865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a subset with the relevant location feature from the location dataset\n",
    "locations_subset = df3_location_dataset[['country', 'locid']]\n",
    "\n",
    "# here we add the location subset (locid and country) to the people dataset. So we add an country to every location id.\n",
    "df_people_dataset = pd.merge(df_people_dataset, locations_subset, on='locid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6faf1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this codeblock we link 'locid', 'country', 'majorarea', 'degrees', 'hindex' and 'fecundity' to both collaborators\n",
    "# first we add this details to the first collaborator (PID1) and then to the seconf (PID2)\n",
    "# we have to rename the column because we merge the dataset based on the pid from the people dataset.\n",
    "\n",
    "df_people_dataset_subset = df_people_dataset[['pid','locid', 'country', 'majorarea', 'degrees', 'hindex', 'fecundity']]\n",
    "\n",
    "df4_connect_dataset.rename(columns={'pid1': 'pid'}, inplace=True)\n",
    "\n",
    "#merge\n",
    "df4_connect_dataset_pid1 = pd.merge(df4_connect_dataset, df_people_dataset_subset, on='pid', how='left')\n",
    "\n",
    "df4_connect_dataset_pid1.rename(columns={'pid': 'pid1'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'degrees': 'degrees1'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'locid_x': 'locid_cid'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'locid_y': 'locid1'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'majorarea': 'majorarea1'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'hindex': 'hindex1'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'country': 'country1'}, inplace=True)\n",
    "df4_connect_dataset_pid1.rename(columns={'fecundity': 'fecundity1'}, inplace=True)\n",
    "\n",
    "# and now for PID2\n",
    "df4_connect_dataset_pid1.rename(columns={'pid2': 'pid'}, inplace=True)\n",
    "\n",
    "df_people_dataset_subset2 = df_people_dataset[['pid','locid', 'country', 'majorarea', 'degrees', 'hindex', 'fecundity']]\n",
    "\n",
    "merged_pid1_and_pid2 = pd.merge(df4_connect_dataset_pid1, df_people_dataset_subset2, on='pid', how='left')\n",
    "\n",
    "merged_pid1_and_pid2.rename(columns={'pid': 'pid2'}, inplace=True)\n",
    "merged_pid1_and_pid2.rename(columns={'degrees': 'degrees2'}, inplace=True)\n",
    "merged_pid1_and_pid2.rename(columns={'locid': 'locid2'}, inplace=True)\n",
    "merged_pid1_and_pid2.rename(columns={'majorarea': 'majorarea2'}, inplace=True)\n",
    "merged_pid1_and_pid2.rename(columns={'hindex': 'hindex2'}, inplace=True)\n",
    "merged_pid1_and_pid2.rename(columns={'country': 'country2'}, inplace=True)\n",
    "merged_pid1_and_pid2.rename(columns={'fecundity': 'fecundity2'}, inplace=True)\n",
    "\n",
    "thesis_dataset = merged_pid1_and_pid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b51b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename fecundity\n",
    "thesis_dataset.drop(columns=['fecundity_y'], inplace=True)\n",
    "thesis_dataset.rename(columns={'fecundity_x': 'fecundity1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the non-collaborations/open ends in the tree (PID2 is 0)\n",
    "thesis_dataset = thesis_dataset[thesis_dataset['pid2'] != 0]\n",
    "\n",
    "# deleting the collaborations who does not include two countries\n",
    "thesis_dataset = thesis_dataset[thesis_dataset['country1'].notna() & thesis_dataset['country2'].notna()]\n",
    "\n",
    "# fill up all the fecundity2 with NA, by 0 (because NA means they do not have children and their fecundity is 0)\n",
    "thesis_dataset['fecundity2']=thesis_dataset['fecundity2'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "161076ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the countries\n",
    "%run cleaning.ipynb\n",
    "thesis_dataset = clean_countries(thesis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88087bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a new variable: region. We map the countries into regions.\n",
    "\n",
    "# 1 is North America\n",
    "# 2 is Europe\n",
    "# 3 is Asia\n",
    "# 4 is Africa\n",
    "# 5 is South America-Carribean-Central America\n",
    "# 6 is Middle East and North Africa\n",
    "# 7 is Oceania\n",
    "\n",
    "\n",
    "region_mapping = {\n",
    "    'United States': 1,\n",
    "    'Canada': 1,\n",
    "    'United Kingdom': 2,\n",
    "    'Germany': 2,\n",
    "    'Netherlands': 2,\n",
    "    'France': 2,\n",
    "    'Hong Kong': 3,\n",
    "    'Australia': 7,\n",
    "    'Switzerland': 2,\n",
    "    'Israel': 6,\n",
    "    'China': 3,\n",
    "    'Japan': 3,\n",
    "    'Sweden': 2,\n",
    "    'Spain': 2,\n",
    "    'Brazil': 5,\n",
    "    'Italy': 2,\n",
    "    'India': 3,\n",
    "    'South Korea': 3,\n",
    "    'Belgium': 2,\n",
    "    'Austria': 2,\n",
    "    'Hungary': 2,\n",
    "    'Denmark': 2,\n",
    "    'Singapore': 3,\n",
    "    'New Zealand': 7,\n",
    "    'Taiwan': 3,\n",
    "    'Finland': 2,\n",
    "    'Mexico': 5,\n",
    "    'Greece': 2,\n",
    "    'Ireland': 2,\n",
    "    'Norway': 2,\n",
    "    'Argentina': 5,\n",
    "    'Russia': 3,\n",
    "    'Chile': 5,\n",
    "    'Turkey': 6,\n",
    "    'Poland': 2,\n",
    "    'Portugal': 2,\n",
    "    'South Africa':4,\n",
    "    'Czechia': 2,\n",
    "    'Puerto Rico': 5,\n",
    "    'Iran': 6,\n",
    "    'Colombia': 5,\n",
    "    'Thailand': 3,\n",
    "    'Iceland': 2,\n",
    "    'Cyprus': 2,\n",
    "    'Slovakia': 2,\n",
    "    'Cameroon': 4,\n",
    "    'Venezuela': 5,\n",
    "    'Philippines': 3,\n",
    "    'Cuba': 5,\n",
    "    'Korea': 3,\n",
    "    'Ukraine': 2,\n",
    "    'Estonia': 2,\n",
    "    'Guam': 3,\n",
    "    'Kuwait': 6,\n",
    "    'Egypt': 6,\n",
    "    'Uruguay': 5,\n",
    "    'Bangladesh': 3,\n",
    "    'Croatia': 2,\n",
    "    'Indonesia': 3,\n",
    "    'Romania': 2,\n",
    "    'Malaysia': 3,\n",
    "    'Slovenia': 2,\n",
    "    'Lebanon': 6,\n",
    "    'U.S. Virgin Islands': 5,\n",
    "    'Panama': 5,\n",
    "    'United Arab Emirates': 6,\n",
    "    'Tunisia': 6,\n",
    "    'Macedonia (FYROM)': 2,\n",
    "    'Grenada': 5,\n",
    "    'Vatican City': 2,\n",
    "    'Georgia': 6,\n",
    "    'Jordan': 6,\n",
    "    'Nepal': 3,\n",
    "    'Jamaica': 5,\n",
    "    'Liberia': 4,\n",
    "    'The Gambia': 4,\n",
    "    'Armenia': 6,\n",
    "    'Costa Rica': 5,\n",
    "    'Uganda': 4,\n",
    "    'Bulgaria': 2,\n",
    "    'Kyrgyzstan': 3,\n",
    "}\n",
    "\n",
    "thesis_dataset['region1'] = thesis_dataset['country1'].map(region_mapping)\n",
    "thesis_dataset['region2'] = thesis_dataset['country2'].map(region_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b19243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create another variable based on membership of the OECD\n",
    "\n",
    "OECD_mapping = {\n",
    "    'United States': 1,\n",
    "    'Canada': 1,\n",
    "    'United Kingdom': 1,\n",
    "    'Germany': 1,\n",
    "    'Netherlands': 1,\n",
    "    'France': 1,\n",
    "    'Hong Kong': 0,\n",
    "    'Australia': 1,\n",
    "    'Switzerland': 1,\n",
    "    'Israel': 1,\n",
    "    'China': 0,\n",
    "    'Japan': 1,\n",
    "    'Sweden': 1,\n",
    "    'Spain': 1,\n",
    "    'Brazil': 0,\n",
    "    'Italy': 1,\n",
    "    'India': 0,\n",
    "    'South Korea': 0,\n",
    "    'Belgium': 1,\n",
    "    'Austria': 1,\n",
    "    'Hungary': 1,\n",
    "    'Denmark': 1,\n",
    "    'Singapore': 0,\n",
    "    'New Zealand': 1,\n",
    "    'Taiwan': 0,\n",
    "    'Finland': 1,\n",
    "    'Mexico': 1,\n",
    "    'Greece': 1,\n",
    "    'Ireland': 1,\n",
    "    'Norway': 1,\n",
    "    'Argentina': 0,\n",
    "    'Russia': 0,\n",
    "    'Chile': 1,\n",
    "    'Turkey': 1,\n",
    "    'Poland': 1,\n",
    "    'Portugal': 1,\n",
    "    'South Africa':0,\n",
    "    'Czechia': 1,\n",
    "    'Puerto Rico': 0,\n",
    "    'Iran': 0,\n",
    "    'Colombia': 1,   \n",
    "    'Thailand': 0,\n",
    "    'Iceland': 1,\n",
    "    'Cyprus': 0,\n",
    "    'Slovakia': 1,\n",
    "    'Cameroon': 0,\n",
    "    'Venezuela': 0,\n",
    "    'Philippines': 0,\n",
    "    'Cuba': 0,\n",
    "    'Korea': 0,\n",
    "    'Ukraine': 0,\n",
    "    'Estonia': 1,\n",
    "    'Guam': 0,\n",
    "    'Kuwait': 0,\n",
    "    'Egypt': 0,\n",
    "    'Uruguay': 0,\n",
    "    'Bangladesh': 0,\n",
    "    'Croatia': 0,\n",
    "    'Indonesia': 0,\n",
    "    'Romania': 0,\n",
    "    'Malaysia': 0,\n",
    "    'Slovenia': 1,\n",
    "    'Lebanon': 0,\n",
    "    'U.S. Virgin Islands': 0,\n",
    "    'Panama': 0,    \n",
    "    'United Arab Emirates': 0,\n",
    "    'Tunisia': 0,\n",
    "    'Macedonia (FYROM)': 0,\n",
    "    'Grenada': 0,\n",
    "    'Vatican City': 0,\n",
    "    'Georgia': 0,\n",
    "    'Jordan': 0,\n",
    "    'Nepal': 0,\n",
    "    'Jamaica': 0,\n",
    "    'Liberia': 0,\n",
    "    'The Gambia': 0,\n",
    "    'Armenia': 0,\n",
    "    'Costa Rica': 1,\n",
    "    'Uganda': 0,\n",
    "    'Bulgaria': 0,\n",
    "    'Kyrgyzstan':0,\n",
    "}\n",
    "\n",
    "thesis_dataset['OECD1'] = thesis_dataset['country1'].map(OECD_mapping)\n",
    "thesis_dataset['OECD2'] = thesis_dataset['country2'].map(OECD_mapping)\n",
    "\n",
    "# Drop NAs\n",
    "thesis_dataset = thesis_dataset.dropna(subset=['OECD1', 'OECD2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70adec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the target variables based on the international and interregional collaboration\n",
    "thesis_dataset['International collaboration'] = np.where(thesis_dataset['country1'] == thesis_dataset['country2'], 0, 1)\n",
    "thesis_dataset['Interregional collaboration'] = np.where(thesis_dataset['region1'] == thesis_dataset['region2'], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4d3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the degrees\n",
    "%run cleaning.ipynb\n",
    "thesis_dataset = clean_degrees(thesis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29614a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding 'degrees' into three categories \n",
    "categories_degrees = ['PhD', 'Ed.D.']\n",
    "\n",
    "def encode_category(degree):\n",
    "    if degree in categories_degrees:\n",
    "        return degree  \n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "thesis_dataset['degrees1'] = thesis_dataset['degrees1'].apply(encode_category)\n",
    "thesis_dataset['degrees2'] = thesis_dataset['degrees2'].apply(encode_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb90c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding 'fecundity' into catagories to make it more likely there is overlap between both collaborators \n",
    "# 0:'0', 1:'1-5', 2:'6-10', 3:'11-15', 4:'16-20', 5:'21+'\n",
    "\n",
    "category_edge = [(-1), 0, 5, 10, 15, 20, float ('inf')]\n",
    "category_labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "thesis_dataset['fecun_category1'] = pd.cut(thesis_dataset['fecundity1'], bins= category_edge, labels=category_labels).astype(int)\n",
    "thesis_dataset['fecun_category2'] = pd.cut(thesis_dataset['fecundity2'], bins= category_edge, labels=category_labels).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "640e5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For six elements a Jaccard similarity score is made to see how similar both collaborators are.\n",
    "\n",
    "# Jaccard for the amounth of children the collaborators have (in categories)\n",
    "thesis_dataset['jacc_fecun_cat'] = (thesis_dataset['fecun_category1'] == thesis_dataset['fecun_category2']).astype(int)\n",
    "\n",
    "# Jaccard score for country\n",
    "thesis_dataset['jacc_country'] = (thesis_dataset['country1'] == thesis_dataset['country2']).astype(int)\n",
    "\n",
    "# Jaccard for degrees\n",
    "thesis_dataset['jacc_degrees'] = (thesis_dataset['degrees1'] == thesis_dataset['degrees2']).astype(int)\n",
    "\n",
    "# Jaccard score for region\n",
    "thesis_dataset['jacc_region'] = (thesis_dataset['region1'] == thesis_dataset['region2']).astype(int)\n",
    "\n",
    "# Jaccard score for OECD membership\n",
    "thesis_dataset['jacc_OECD'] = (thesis_dataset['OECD1'] == thesis_dataset['OECD2']).astype(int)\n",
    "\n",
    "# Jaccard: overlap for major area\n",
    "def check_overlap(majorarea1, majorarea2):\n",
    "    majorarea1_elements = majorarea1.split(',') \n",
    "    majorarea2_elements = majorarea2.split(',') \n",
    "    \n",
    "    majorarea1_set = set(majorarea1_elements)\n",
    "    majorarea2_set = set(majorarea2_elements)\n",
    "    overlap_exists = len(majorarea1_set.intersection(majorarea2_set)) > 0\n",
    "    \n",
    "    jacc_majorarea = int(overlap_exists)\n",
    "    \n",
    "    return jacc_majorarea\n",
    "\n",
    "# new column 'jacc_majorarea' \n",
    "thesis_dataset['jacc_majorarea'] = thesis_dataset.apply(lambda row: check_overlap(row['majorarea1'], row['majorarea2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8df0df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Jaccard Coefficient \n",
    "thesis_dataset['jaccard_coefficient'] = ( thesis_dataset['jacc_fecun_cat'] + thesis_dataset['jacc_country'] + thesis_dataset['jacc_degrees'] + thesis_dataset['jacc_region']+ thesis_dataset['jacc_OECD'] +thesis_dataset['jacc_majorarea']) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70e24aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some useless variables for clarity, because of an high amounth of NANs or because the information in saved in other variables\n",
    "thesis_dataset.drop(['pid1', 'pid2', 'cid','location', 'locid_cid', 'startdate','stopdate' ,'country1', 'country2', 'majorarea1', \n",
    "          'majorarea2', 'hindex1', 'hindex2', 'locid1', 'locid2',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d899b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "columns_to_encode = ['region1', 'region2','relation', 'fecun_category1', 'fecun_category2','degrees1', 'degrees2']\n",
    "thesis_dataset = pd.get_dummies(thesis_dataset, columns=columns_to_encode, prefix=columns_to_encode, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77748a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the continuous variables\n",
    "features_to_scale = ['fecundity1', 'fecundity2']\n",
    "\n",
    "# With standard scaler\n",
    "scaler = StandardScaler()\n",
    "thesis_dataset[features_to_scale] = scaler.fit_transform(thesis_dataset[features_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01bf69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_dataset['North_America_1'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['region1_2.0'] == 0) &\n",
    "    (thesis_dataset['region1_3.0'] == 0) &\n",
    "    (thesis_dataset['region1_4.0'] == 0) &\n",
    "    (thesis_dataset['region1_5.0'] == 0) &\n",
    "    (thesis_dataset['region1_6.0'] == 0) &\n",
    "    (thesis_dataset['region1_7.0'] == 0)\n",
    ")\n",
    "thesis_dataset.loc[conditions, 'North_America_1'] = 1\n",
    "\n",
    "thesis_dataset['North_America_2'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['region2_2.0'] == 0) &\n",
    "    (thesis_dataset['region2_3.0'] == 0) &\n",
    "    (thesis_dataset['region2_4.0'] == 0) &\n",
    "    (thesis_dataset['region2_5.0'] == 0) &\n",
    "    (thesis_dataset['region2_6.0'] == 0) &\n",
    "    (thesis_dataset['region2_7.0'] == 0)\n",
    ")\n",
    "thesis_dataset.loc[conditions, 'North_America_2'] = 1\n",
    "\n",
    "thesis_dataset['relation_0'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['relation_1'] == 0) &\n",
    "    (thesis_dataset['relation_2'] == 0) &\n",
    "    (thesis_dataset['relation_3'] == 0) &\n",
    "    (thesis_dataset['relation_4'] == 0) \n",
    ")\n",
    "thesis_dataset.loc[conditions, 'relation_0'] = 1\n",
    "\n",
    "thesis_dataset['degrees_edd1'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['degrees1_Other'] == 0) &\n",
    "    (thesis_dataset['degrees1_PhD'] == 0) \n",
    ")\n",
    "thesis_dataset.loc[conditions, 'degrees_edd1'] = 1\n",
    "\n",
    "thesis_dataset['degrees_edd2'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['degrees2_Other'] == 0) &\n",
    "    (thesis_dataset['degrees2_PhD'] == 0) \n",
    ")\n",
    "thesis_dataset.loc[conditions, 'degrees_edd2'] = 1\n",
    "\n",
    "thesis_dataset['fecun_category1_0'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['fecun_category1_2'] == 0) &\n",
    "    (thesis_dataset['fecun_category1_3'] == 0) &\n",
    "    (thesis_dataset['fecun_category1_4'] == 0) &\n",
    "    (thesis_dataset['fecun_category1_5'] == 0) \n",
    ")\n",
    "thesis_dataset.loc[conditions, 'fecun_category1_0'] = 1\n",
    "\n",
    "thesis_dataset['fecun_category2_0'] = 0\n",
    "conditions = (\n",
    "    (thesis_dataset['fecun_category2_1'] == 0) &\n",
    "    (thesis_dataset['fecun_category2_2'] == 0) &\n",
    "    (thesis_dataset['fecun_category2_3'] == 0) &\n",
    "    (thesis_dataset['fecun_category2_4'] == 0) &\n",
    "    (thesis_dataset['fecun_category2_5'] == 0) \n",
    ")\n",
    "thesis_dataset.loc[conditions, 'fecun_category2_0'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bb9df86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10         1\n",
       "37         1\n",
       "38         1\n",
       "40         1\n",
       "42         1\n",
       "          ..\n",
       "1561361    1\n",
       "1561363    1\n",
       "1561365    1\n",
       "1561472    1\n",
       "1561473    1\n",
       "Name: fecun_category2_1, Length: 331384, dtype: uint8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesis_dataset['fecun_category2_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d842810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fecundity1', 'fecundity2', 'OECD1', 'OECD2',\n",
       "       'International collaboration', 'Interregional collaboration',\n",
       "       'jacc_fecun_cat', 'jacc_country', 'jacc_degrees', 'jacc_region',\n",
       "       'jacc_OECD', 'jacc_majorarea', 'jaccard_coefficient', 'region1_2.0',\n",
       "       'region1_3.0', 'region1_4.0', 'region1_5.0', 'region1_6.0',\n",
       "       'region1_7.0', 'region2_2.0', 'region2_3.0', 'region2_4.0',\n",
       "       'region2_5.0', 'region2_6.0', 'region2_7.0', 'relation_1', 'relation_2',\n",
       "       'relation_3', 'relation_4', 'fecun_category1_2', 'fecun_category1_3',\n",
       "       'fecun_category1_4', 'fecun_category1_5', 'fecun_category2_1',\n",
       "       'fecun_category2_2', 'fecun_category2_3', 'fecun_category2_4',\n",
       "       'fecun_category2_5', 'degrees1_Other', 'degrees1_PhD', 'degrees2_Other',\n",
       "       'degrees2_PhD', 'North_America_1', 'North_America_2', 'relation_0',\n",
       "       'degrees_edd1', 'degrees_edd2', 'fecun_category1_0',\n",
       "       'fecun_category2_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesis_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cd81587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits de data voor de eerste target variabele 'International collaboration'\n",
    "y1 = thesis_dataset['International collaboration']\n",
    "\n",
    "# Splits de data gestratificeerd (behoudt de originele verdeling van de target variabele)\n",
    "train1, test1 = train_test_split(thesis_dataset, test_size=0.2, stratify=y1, random_state=101)\n",
    "\n",
    "X_train1 = train1.drop('International collaboration', axis=1)\n",
    "y_train1 = train1['International collaboration']\n",
    "\n",
    "X_test1 = test1.drop('International collaboration', axis=1)\n",
    "y_test1 = test1['International collaboration']\n",
    "\n",
    "# Splits de data voor de tweede target variabele 'Interregional collaboration'\n",
    "y2 = thesis_dataset['Interregional collaboration']\n",
    "\n",
    "# Splits de data gestratificeerd (behoudt de originele verdeling van de target variabele)\n",
    "train2, test2 = train_test_split(thesis_dataset, test_size=0.2, stratify=y2, random_state=101)\n",
    "\n",
    "X_train2 = train2.drop('Interregional collaboration', axis=1)\n",
    "y_train2 = train2['Interregional collaboration']\n",
    "\n",
    "X_test2 = test2.drop('Interregional collaboration', axis=1)\n",
    "y_test2 = test2['Interregional collaboration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "347d856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the highest correlation, five features per target variable are deleted\n",
    "X_train1.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test1.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train2.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test2.drop(['International collaboration','jacc_region'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "869e7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "# Defining the target variable\n",
    "y_target = 'International collaboration'\n",
    "\n",
    "# Empty dictionaries to store subsets\n",
    "X_train_subsets = {}\n",
    "X_test_subsets = {}\n",
    "y_train_subsets = {}\n",
    "y_test_subsets = {}\n",
    "\n",
    "# Iterating through the subsets\n",
    "for i in range(1, 8):\n",
    "    subset_name = f'subset_{i}'\n",
    "\n",
    "    # Extracting features from the subset\n",
    "    X_subset_i = thesis_dataset[\n",
    "        (thesis_dataset['region1_2.0'] == (i == 2)) &\n",
    "        (thesis_dataset['region1_3.0'] == (i == 3)) &\n",
    "        (thesis_dataset['region1_4.0'] == (i == 4)) &\n",
    "        (thesis_dataset['region1_5.0'] == (i == 5)) &\n",
    "        (thesis_dataset['region1_6.0'] == (i == 6)) &\n",
    "        (thesis_dataset['region1_7.0'] == (i == 7))\n",
    "    ]\n",
    "\n",
    "    # Extracting target variable\n",
    "    y_subset_i = X_subset_i[y_target]\n",
    "\n",
    "    # Dropping the target variable from features\n",
    "    X_subset_i = X_subset_i.drop(columns=[y_target])\n",
    "\n",
    "    # Create a StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Get the indices for train and test sets using stratified sampling\n",
    "    train_index, test_index = next(sss.split(X_subset_i, y_subset_i))\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train_subset_i, X_test_subset_i = X_subset_i.iloc[train_index], X_subset_i.iloc[test_index]\n",
    "    y_train_subset_i, y_test_subset_i = y_subset_i.iloc[train_index], y_subset_i.iloc[test_index]\n",
    "\n",
    "    # Store subsets in dictionaries\n",
    "    X_train_subsets[subset_name] = X_train_subset_i\n",
    "    X_test_subsets[subset_name] = X_test_subset_i\n",
    "    y_train_subsets[subset_name] = y_train_subset_i\n",
    "    y_test_subsets[subset_name] = y_test_subset_i\n",
    "\n",
    "    # Save them\n",
    "    results.append({\n",
    "        'subset_name': subset_name,\n",
    "        'X_train': X_train_subset_i,\n",
    "        'X_test': X_test_subset_i,\n",
    "        'y_train': y_train_subset_i,\n",
    "        'y_test': y_test_subset_i\n",
    "    })\n",
    "\n",
    "# The subsets outside the loop\n",
    "X_train_subset_1 = X_train_subsets['subset_1']\n",
    "X_test_subset_1 = X_test_subsets['subset_1']\n",
    "y_train_subset_1 = y_train_subsets['subset_1']\n",
    "y_test_subset_1 = y_test_subsets['subset_1']\n",
    "\n",
    "X_train_subset_2 = X_train_subsets['subset_2']\n",
    "X_test_subset_2 = X_test_subsets['subset_2']\n",
    "y_train_subset_2 = y_train_subsets['subset_2']\n",
    "y_test_subset_2 = y_test_subsets['subset_2']\n",
    "\n",
    "X_train_subset_3 = X_train_subsets['subset_3']\n",
    "X_test_subset_3 = X_test_subsets['subset_3']\n",
    "y_train_subset_3 = y_train_subsets['subset_3']\n",
    "y_test_subset_3 = y_test_subsets['subset_3']\n",
    "\n",
    "X_train_subset_4 = X_train_subsets['subset_4']\n",
    "X_test_subset_4 = X_test_subsets['subset_4']\n",
    "y_train_subset_4 = y_train_subsets['subset_4']\n",
    "y_test_subset_4 = y_test_subsets['subset_4']\n",
    "\n",
    "X_train_subset_5 = X_train_subsets['subset_5']\n",
    "X_test_subset_5 = X_test_subsets['subset_5']\n",
    "y_train_subset_5 = y_train_subsets['subset_5']\n",
    "y_test_subset_5 = y_test_subsets['subset_5']\n",
    "\n",
    "X_train_subset_6 = X_train_subsets['subset_6']\n",
    "X_test_subset_6 = X_test_subsets['subset_6']\n",
    "y_train_subset_6 = y_train_subsets['subset_6']\n",
    "y_test_subset_6 = y_test_subsets['subset_6']\n",
    "\n",
    "X_train_subset_7 = X_train_subsets['subset_7']\n",
    "X_test_subset_7 = X_test_subsets['subset_7']\n",
    "y_train_subset_7 = y_train_subsets['subset_7']\n",
    "y_test_subset_7 = y_test_subsets['subset_7']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c0d48ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mhell\\AppData\\Local\\Temp\\ipykernel_4884\\3040456052.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_subset_7.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
      "C:\\Users\\Mhell\\AppData\\Local\\Temp\\ipykernel_4884\\3040456052.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_subset_7.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# other targetvariable deleted and 'jacc_country' deleted because of the overlap with the target variable.\n",
    "X_train_subset_1.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_1.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train_subset_2.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_2.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train_subset_3.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_3.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train_subset_4.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_4.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train_subset_5.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_5.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train_subset_6.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_6.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_train_subset_7.drop([ 'Interregional collaboration','jacc_country'], axis=1, inplace=True)\n",
    "X_test_subset_7.drop(['Interregional collaboration','jacc_country'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0bcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "# Defining the target variable\n",
    "y_target = 'Interregional collaboration'\n",
    "\n",
    "# Empty dictionaries to store subsets\n",
    "X_train_subsets_2_ = {}\n",
    "X_test_subsets_2_ = {}\n",
    "y_train_subsets_2_ = {}\n",
    "y_test_subsets_2_ = {}\n",
    "\n",
    "# Iterating through the subsets\n",
    "for i in range(1, 8):\n",
    "    subset_name = f'subset_2_{i}'\n",
    "\n",
    "    # Extracting features from the subset\n",
    "    X_subset_2_i = thesis_dataset[\n",
    "        (thesis_dataset['region1_2.0'] == (i == 2)) &\n",
    "        (thesis_dataset['region1_3.0'] == (i == 3)) &\n",
    "        (thesis_dataset['region1_4.0'] == (i == 4)) &\n",
    "        (thesis_dataset['region1_5.0'] == (i == 5)) &\n",
    "        (thesis_dataset['region1_6.0'] == (i == 6)) &\n",
    "        (thesis_dataset['region1_7.0'] == (i == 7))\n",
    "    ]\n",
    "\n",
    "    # Extracting target variable\n",
    "    y_subset_2_i = X_subset_2_i[y_target]\n",
    "\n",
    "    # Dropping the target variable from features\n",
    "    X_subset_2_i = X_subset_2_i.drop(columns=[y_target])\n",
    "\n",
    "    # Create a StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Get the indices for train and test sets using stratified sampling\n",
    "    train_index, test_index = next(sss.split(X_subset_2_i, y_subset_2_i))\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train_subset_2_i, X_test_subset_2_i = X_subset_2_i.iloc[train_index], X_subset_2_i.iloc[test_index]\n",
    "    y_train_subset_2_i, y_test_subset_2_i = y_subset_2_i.iloc[train_index], y_subset_2_i.iloc[test_index]\n",
    "\n",
    "    # Store subsets in dictionaries\n",
    "    X_train_subsets_2_[subset_name] = X_train_subset_2_i\n",
    "    X_test_subsets_2_[subset_name] = X_test_subset_2_i\n",
    "    y_train_subsets_2_[subset_name] = y_train_subset_2_i\n",
    "    y_test_subsets_2_[subset_name] = y_test_subset_2_i\n",
    "\n",
    "    # Save them\n",
    "    results.append({\n",
    "        'subset_name': subset_name,\n",
    "        'X_train': X_train_subset_2_i,\n",
    "        'X_test': X_test_subset_2_i,\n",
    "        'y_train': y_train_subset_2_i,\n",
    "        'y_test': y_test_subset_2_i\n",
    "    })\n",
    "\n",
    "# The subsets outside the loop\n",
    "X_train_subset_2_1 = X_train_subsets_2_['subset_2_1']\n",
    "X_test_subset_2_1 = X_test_subsets_2_['subset_2_1']\n",
    "y_train_subset_2_1 = y_train_subsets_2_['subset_2_1']\n",
    "y_test_subset_2_1 = y_test_subsets_2_['subset_2_1']\n",
    "\n",
    "X_train_subset_2_2 = X_train_subsets_2_['subset_2_2']\n",
    "X_test_subset_2_2 = X_test_subsets_2_['subset_2_2']\n",
    "y_train_subset_2_2 = y_train_subsets_2_['subset_2_2']\n",
    "y_test_subset_2_2 = y_test_subsets_2_['subset_2_2']\n",
    "\n",
    "X_train_subset_2_3 = X_train_subsets_2_['subset_2_3']\n",
    "X_test_subset_2_3 = X_test_subsets_2_['subset_2_3']\n",
    "y_train_subset_2_3 = y_train_subsets_2_['subset_2_3']\n",
    "y_test_subset_2_3 = y_test_subsets_2_['subset_2_3']\n",
    "\n",
    "X_train_subset_2_4 = X_train_subsets_2_['subset_2_4']\n",
    "X_test_subset_2_4 = X_test_subsets_2_['subset_2_4']\n",
    "y_train_subset_2_4 = y_train_subsets_2_['subset_2_4']\n",
    "y_test_subset_2_4 = y_test_subsets_2_['subset_2_4']\n",
    "\n",
    "X_train_subset_2_5 = X_train_subsets_2_['subset_2_5']\n",
    "X_test_subset_2_5 = X_test_subsets_2_['subset_2_5']\n",
    "y_train_subset_2_5 = y_train_subsets_2_['subset_2_5']\n",
    "y_test_subset_2_5 = y_test_subsets_2_['subset_2_5']\n",
    "\n",
    "X_train_subset_2_6 = X_train_subsets_2_['subset_2_6']\n",
    "X_test_subset_2_6 = X_test_subsets_2_['subset_2_6']\n",
    "y_train_subset_2_6 = y_train_subsets_2_['subset_2_6']\n",
    "y_test_subset_2_6 = y_test_subsets_2_['subset_2_6']\n",
    "\n",
    "X_train_subset_2_7 = X_train_subsets_2_['subset_2_7']\n",
    "X_test_subset_2_7 = X_test_subsets_2_['subset_2_7']\n",
    "y_train_subset_2_7 = y_train_subsets_2_['subset_2_7']\n",
    "y_test_subset_2_7 = y_test_subsets_2_['subset_2_7']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dfe1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mhell\\AppData\\Local\\Temp\\ipykernel_4884\\2286305852.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_subset_2_7.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
      "C:\\Users\\Mhell\\AppData\\Local\\Temp\\ipykernel_4884\\2286305852.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_subset_2_7.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# other targetvariable deleted and 'jacc_region' deleted because of the overlap with the target variable.\n",
    "X_train_subset_2_1.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_1.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n",
    "X_train_subset_2_2.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_2.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n",
    "X_train_subset_2_3.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_3.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n",
    "X_train_subset_2_4.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_4.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n",
    "X_train_subset_2_5.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_5.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n",
    "X_train_subset_2_6.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_6.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n",
    "X_train_subset_2_7.drop([ 'International collaboration', 'jacc_region'], axis=1, inplace=True)\n",
    "X_test_subset_2_7.drop(['International collaboration','jacc_region'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db6ff834",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables with importance for North America >= 0.001:\n",
      "                Feature  Importance\n",
      "9   jaccard_coefficient    0.239414\n",
      "40      North_America_2    0.206453\n",
      "6           jacc_region    0.175656\n",
      "16          region2_2.0    0.147909\n",
      "8        jacc_majorarea    0.078179\n",
      "0            fecundity1    0.029189\n",
      "5          jacc_degrees    0.014694\n",
      "3                 OECD2    0.014580\n",
      "7             jacc_OECD    0.010572\n",
      "17          region2_3.0    0.010387\n",
      "1            fecundity2    0.010107\n",
      "4        jacc_fecun_cat    0.009931\n",
      "35       degrees1_Other    0.007366\n",
      "21          region2_7.0    0.007170\n",
      "30    fecun_category2_1    0.005868\n",
      "36         degrees1_PhD    0.005576\n",
      "22           relation_1    0.005346\n",
      "45    fecun_category2_0    0.003588\n",
      "38         degrees2_PhD    0.003491\n",
      "44    fecun_category1_0    0.002674\n",
      "20          region2_6.0    0.002430\n",
      "37       degrees2_Other    0.001912\n",
      "31    fecun_category2_2    0.001545\n",
      "26    fecun_category1_2    0.001394\n",
      "23           relation_2    0.001031\n",
      "\n",
      "Variables with importance for Europe >= 0.001:\n",
      "                Feature  Importance\n",
      "9   jaccard_coefficient    0.395175\n",
      "16          region2_2.0    0.121312\n",
      "6           jacc_region    0.110288\n",
      "40      North_America_2    0.083342\n",
      "8        jacc_majorarea    0.063664\n",
      "5          jacc_degrees    0.034029\n",
      "0            fecundity1    0.026505\n",
      "4        jacc_fecun_cat    0.022728\n",
      "22           relation_1    0.018546\n",
      "1            fecundity2    0.016423\n",
      "30    fecun_category2_1    0.015577\n",
      "37       degrees2_Other    0.013290\n",
      "38         degrees2_PhD    0.011316\n",
      "35       degrees1_Other    0.010844\n",
      "23           relation_2    0.010257\n",
      "36         degrees1_PhD    0.008296\n",
      "21          region2_7.0    0.007049\n",
      "45    fecun_category2_0    0.005443\n",
      "44    fecun_category1_0    0.003960\n",
      "31    fecun_category2_2    0.003026\n",
      "26    fecun_category1_2    0.002954\n",
      "7             jacc_OECD    0.002837\n",
      "3                 OECD2    0.002773\n",
      "25           relation_4    0.002743\n",
      "17          region2_3.0    0.002162\n",
      "20          region2_6.0    0.001325\n",
      "\n",
      "Variables with importance for Asia >= 0.001:\n",
      "                Feature  Importance\n",
      "17          region2_3.0    0.258170\n",
      "9   jaccard_coefficient    0.161679\n",
      "6           jacc_region    0.157938\n",
      "7             jacc_OECD    0.105734\n",
      "40      North_America_2    0.091698\n",
      "3                 OECD2    0.082290\n",
      "16          region2_2.0    0.037902\n",
      "23           relation_2    0.032698\n",
      "0            fecundity1    0.013368\n",
      "1            fecundity2    0.011588\n",
      "22           relation_1    0.007835\n",
      "4        jacc_fecun_cat    0.006415\n",
      "30    fecun_category2_1    0.005697\n",
      "2                 OECD1    0.005566\n",
      "45    fecun_category2_0    0.004695\n",
      "36         degrees1_PhD    0.002895\n",
      "8        jacc_majorarea    0.002511\n",
      "5          jacc_degrees    0.002186\n",
      "35       degrees1_Other    0.001614\n",
      "37       degrees2_Other    0.001461\n",
      "38         degrees2_PhD    0.001392\n",
      "25           relation_4    0.001029\n",
      "26    fecun_category1_2    0.001028\n",
      "Variables with importance for Africa >= 0.001:\n",
      "                Feature  Importance\n",
      "6           jacc_region    0.223694\n",
      "9   jaccard_coefficient    0.207449\n",
      "3                 OECD2    0.117046\n",
      "18          region2_4.0    0.114264\n",
      "16          region2_2.0    0.096589\n",
      "0            fecundity1    0.083663\n",
      "7             jacc_OECD    0.076888\n",
      "40      North_America_2    0.025380\n",
      "38         degrees2_PhD    0.013834\n",
      "37       degrees2_Other    0.010808\n",
      "22           relation_1    0.008723\n",
      "32    fecun_category2_3    0.006950\n",
      "1            fecundity2    0.005879\n",
      "36         degrees1_PhD    0.002626\n",
      "35       degrees1_Other    0.002422\n",
      "26    fecun_category1_2    0.001803\n",
      "4        jacc_fecun_cat    0.001521\n",
      "\n",
      "Variables with importance for Latin America >= 0.001:\n",
      "                Feature  Importance\n",
      "6           jacc_region    0.285779\n",
      "19          region2_5.0    0.229152\n",
      "9   jaccard_coefficient    0.178706\n",
      "7             jacc_OECD    0.080555\n",
      "40      North_America_2    0.080329\n",
      "3                 OECD2    0.050224\n",
      "16          region2_2.0    0.037880\n",
      "23           relation_2    0.009503\n",
      "0            fecundity1    0.006936\n",
      "22           relation_1    0.006812\n",
      "1            fecundity2    0.004558\n",
      "2                 OECD1    0.004460\n",
      "31    fecun_category2_2    0.003305\n",
      "5          jacc_degrees    0.003038\n",
      "4        jacc_fecun_cat    0.002883\n",
      "8        jacc_majorarea    0.002427\n",
      "30    fecun_category2_1    0.001870\n",
      "36         degrees1_PhD    0.001509\n",
      "37       degrees2_Other    0.001491\n",
      "38         degrees2_PhD    0.001385\n",
      "35       degrees1_Other    0.001161\n",
      "44    fecun_category1_0    0.001012\n",
      "\n",
      "Variables with importance for Arab countries >= 0.001:\n",
      "                Feature  Importance\n",
      "6           jacc_region    0.309175\n",
      "20          region2_6.0    0.301691\n",
      "9   jaccard_coefficient    0.139411\n",
      "40      North_America_2    0.093803\n",
      "16          region2_2.0    0.050627\n",
      "22           relation_1    0.023929\n",
      "23           relation_2    0.012123\n",
      "0            fecundity1    0.009309\n",
      "7             jacc_OECD    0.009239\n",
      "1            fecundity2    0.008724\n",
      "45    fecun_category2_0    0.006550\n",
      "5          jacc_degrees    0.005670\n",
      "4        jacc_fecun_cat    0.004678\n",
      "35       degrees1_Other    0.004233\n",
      "30    fecun_category2_1    0.003808\n",
      "2                 OECD1    0.003024\n",
      "36         degrees1_PhD    0.003006\n",
      "25           relation_4    0.002026\n",
      "17          region2_3.0    0.001844\n",
      "\n",
      "Variables with importance >= 0.001:\n",
      "                Feature  Importance\n",
      "21          region2_7.0    0.299345\n",
      "6           jacc_region    0.215488\n",
      "9   jaccard_coefficient    0.210774\n",
      "16          region2_2.0    0.103281\n",
      "40      North_America_2    0.093645\n",
      "23           relation_2    0.010734\n",
      "1            fecundity2    0.008600\n",
      "22           relation_1    0.007616\n",
      "30    fecun_category2_1    0.006588\n",
      "0            fecundity1    0.005714\n",
      "4        jacc_fecun_cat    0.004257\n",
      "37       degrees2_Other    0.003790\n",
      "8        jacc_majorarea    0.003727\n",
      "5          jacc_degrees    0.003337\n",
      "36         degrees1_PhD    0.003128\n",
      "35       degrees1_Other    0.002796\n",
      "45    fecun_category2_0    0.002735\n",
      "31    fecun_category2_2    0.002418\n",
      "44    fecun_category1_0    0.002034\n",
      "38         degrees2_PhD    0.001908\n",
      "7             jacc_OECD    0.001686\n",
      "3                 OECD2    0.001442\n"
     ]
    }
   ],
   "source": [
    "# Feature importance per subset, everything lower than 0.001 is deleted.\n",
    "# Subset 1\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_1, y_train_subset_1)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_1, y_train_subset_1)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_1.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for North America >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_1 = X_train_subset_1.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 2\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2, y_train_subset_2)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2, y_train_subset_2)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Europe >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2 = X_train_subset_2.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 3\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_3, y_train_subset_3)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_3, y_train_subset_3)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_3.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Asia >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_3 = X_train_subset_3.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 4\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_4, y_train_subset_4)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_4, y_train_subset_4)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_4.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Africa >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_4 = X_train_subset_4.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 5\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_5, y_train_subset_5)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_5, y_train_subset_5)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_5.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Latin America >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_5 = X_train_subset_5.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 6\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_6, y_train_subset_6)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_6, y_train_subset_6)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_6.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Arab countries >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_6 = X_train_subset_6.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 7 \n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_7, y_train_subset_7)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_7, y_train_subset_7)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_7.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_7 = X_train_subset_7.drop(columns=low_importance_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dad8dfc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables with importance for North America >= 0.001:\n",
      "                Feature  Importance\n",
      "40      North_America_2    0.392506\n",
      "16          region2_2.0    0.212865\n",
      "5          jacc_country    0.157746\n",
      "9   jaccard_coefficient    0.120595\n",
      "3                 OECD2    0.018295\n",
      "17          region2_3.0    0.016955\n",
      "7             jacc_OECD    0.016490\n",
      "0            fecundity1    0.010843\n",
      "21          region2_7.0    0.010776\n",
      "20          region2_6.0    0.007768\n",
      "6          jacc_degrees    0.005180\n",
      "8        jacc_majorarea    0.004117\n",
      "30    fecun_category2_1    0.004043\n",
      "19          region2_5.0    0.003598\n",
      "35       degrees1_Other    0.003393\n",
      "4        jacc_fecun_cat    0.003190\n",
      "1            fecundity2    0.002635\n",
      "22           relation_1    0.002200\n",
      "45    fecun_category2_0    0.001786\n",
      "36         degrees1_PhD    0.001554\n",
      "\n",
      "Variables with importance for Europe >= 0.001:\n",
      "                Feature  Importance\n",
      "16          region2_2.0    0.441887\n",
      "40      North_America_2    0.261790\n",
      "9   jaccard_coefficient    0.133132\n",
      "5          jacc_country    0.069926\n",
      "21          region2_7.0    0.017896\n",
      "7             jacc_OECD    0.011960\n",
      "3                 OECD2    0.009900\n",
      "17          region2_3.0    0.009815\n",
      "6          jacc_degrees    0.004743\n",
      "20          region2_6.0    0.004734\n",
      "4        jacc_fecun_cat    0.004398\n",
      "23           relation_2    0.004063\n",
      "22           relation_1    0.003847\n",
      "1            fecundity2    0.002977\n",
      "30    fecun_category2_1    0.002910\n",
      "19          region2_5.0    0.002528\n",
      "45    fecun_category2_0    0.002370\n",
      "8        jacc_majorarea    0.001571\n",
      "36         degrees1_PhD    0.001477\n",
      "38         degrees2_PhD    0.001274\n",
      "0            fecundity1    0.001274\n",
      "37       degrees2_Other    0.001121\n",
      "\n",
      "Variables with importance for Asia >= 0.001:\n",
      "                Feature  Importance\n",
      "17          region2_3.0    0.258205\n",
      "5          jacc_country    0.162413\n",
      "9   jaccard_coefficient    0.138085\n",
      "7             jacc_OECD    0.123630\n",
      "40      North_America_2    0.112094\n",
      "3                 OECD2    0.093139\n",
      "16          region2_2.0    0.031571\n",
      "23           relation_2    0.023138\n",
      "22           relation_1    0.014758\n",
      "2                 OECD1    0.007897\n",
      "0            fecundity1    0.007303\n",
      "45    fecun_category2_0    0.006304\n",
      "1            fecundity2    0.005483\n",
      "4        jacc_fecun_cat    0.003692\n",
      "30    fecun_category2_1    0.003402\n",
      "6          jacc_degrees    0.001641\n",
      "36         degrees1_PhD    0.001081\n",
      "38         degrees2_PhD    0.001036\n",
      "Variables with importance for Africa >= 0.001:\n",
      "                Feature  Importance\n",
      "9   jaccard_coefficient    0.239801\n",
      "5          jacc_country    0.178130\n",
      "7             jacc_OECD    0.149721\n",
      "3                 OECD2    0.139783\n",
      "18          region2_4.0    0.125970\n",
      "16          region2_2.0    0.051435\n",
      "0            fecundity1    0.027766\n",
      "40      North_America_2    0.018867\n",
      "37       degrees2_Other    0.014646\n",
      "38         degrees2_PhD    0.013179\n",
      "1            fecundity2    0.008130\n",
      "22           relation_1    0.006924\n",
      "32    fecun_category2_3    0.003649\n",
      "6          jacc_degrees    0.003357\n",
      "44    fecun_category1_0    0.002684\n",
      "35       degrees1_Other    0.002317\n",
      "17          region2_3.0    0.002122\n",
      "36         degrees1_PhD    0.001682\n",
      "45    fecun_category2_0    0.001245\n",
      "8        jacc_majorarea    0.001217\n",
      "30    fecun_category2_1    0.001182\n",
      "21          region2_7.0    0.001143\n",
      "24           relation_3    0.001103\n",
      "\n",
      "Variables with importance for Latin America >= 0.001:\n",
      "                Feature  Importance\n",
      "19          region2_5.0    0.357040\n",
      "5          jacc_country    0.229414\n",
      "9   jaccard_coefficient    0.161601\n",
      "7             jacc_OECD    0.075276\n",
      "40      North_America_2    0.063272\n",
      "16          region2_2.0    0.041702\n",
      "3                 OECD2    0.033082\n",
      "23           relation_2    0.009222\n",
      "2                 OECD1    0.005453\n",
      "22           relation_1    0.003968\n",
      "0            fecundity1    0.003324\n",
      "31    fecun_category2_2    0.002246\n",
      "37       degrees2_Other    0.002208\n",
      "6          jacc_degrees    0.002014\n",
      "8        jacc_majorarea    0.001697\n",
      "36         degrees1_PhD    0.001391\n",
      "\n",
      "Variables with importance for Arab countries >= 0.001:\n",
      "                Feature  Importance\n",
      "20          region2_6.0    0.293633\n",
      "5          jacc_country    0.281471\n",
      "9   jaccard_coefficient    0.147902\n",
      "40      North_America_2    0.113491\n",
      "16          region2_2.0    0.051285\n",
      "22           relation_1    0.029058\n",
      "23           relation_2    0.016964\n",
      "0            fecundity1    0.012302\n",
      "7             jacc_OECD    0.010966\n",
      "6          jacc_degrees    0.006050\n",
      "1            fecundity2    0.005493\n",
      "36         degrees1_PhD    0.004684\n",
      "45    fecun_category2_0    0.004180\n",
      "30    fecun_category2_1    0.003773\n",
      "4        jacc_fecun_cat    0.003423\n",
      "2                 OECD1    0.002783\n",
      "35       degrees1_Other    0.002513\n",
      "17          region2_3.0    0.001604\n",
      "25           relation_4    0.001541\n",
      "44    fecun_category1_0    0.001363\n",
      "37       degrees2_Other    0.001175\n",
      "\n",
      "Variables with importance >= 0.001:\n",
      "                Feature  Importance\n",
      "21          region2_7.0    0.342700\n",
      "5          jacc_country    0.221835\n",
      "9   jaccard_coefficient    0.188961\n",
      "16          region2_2.0    0.102253\n",
      "40      North_America_2    0.090413\n",
      "23           relation_2    0.009361\n",
      "22           relation_1    0.007033\n",
      "1            fecundity2    0.004769\n",
      "4        jacc_fecun_cat    0.003742\n",
      "30    fecun_category2_1    0.003323\n",
      "6          jacc_degrees    0.003283\n",
      "3                 OECD2    0.003114\n",
      "7             jacc_OECD    0.002523\n",
      "0            fecundity1    0.002435\n",
      "17          region2_3.0    0.001744\n",
      "31    fecun_category2_2    0.001614\n",
      "37       degrees2_Other    0.001297\n",
      "35       degrees1_Other    0.001086\n"
     ]
    }
   ],
   "source": [
    "# Feature importance per subset, everything lower than 0.001 is deleted.\n",
    "# Subset 1\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_1, y_train_subset_2_1)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_1, y_train_subset_2_1)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_1.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for North America >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_1 = X_train_subset_2_1.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 2\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_2, y_train_subset_2_2)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_2, y_train_subset_2_2)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_2.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Europe >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_2 = X_train_subset_2_2.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 3\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_3, y_train_subset_2_3)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_3, y_train_subset_2_3)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_3.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Asia >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_3 = X_train_subset_2_3.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 4\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_4, y_train_subset_2_4)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_4, y_train_subset_2_4)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_4.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Africa >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_4 = X_train_subset_2_4.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 5\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_5, y_train_subset_2_5)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_5, y_train_subset_2_5)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_5.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Latin America >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_5 = X_train_subset_2_5.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 6\n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_6, y_train_subset_2_6)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_6, y_train_subset_2_6)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_6.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance for Arab countries >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "print()\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_6 = X_train_subset_2_6.drop(columns=low_importance_variables)\n",
    "\n",
    "\n",
    "# Subset 7 \n",
    "param_grid = {'n_estimators': [int(x) for x in range(50, 251, 50)],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]}\n",
    "\n",
    "# Initialize the RandomForestClassifier (use it with the best hyperparameters), RandomizedSearchCV (add it on the training data)\n",
    "# Then fit the model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=3, cv=5, random_state=42)\n",
    "random_search.fit(X_train_subset_2_7, y_train_subset_2_7)\n",
    "best_params = random_search.best_params_\n",
    "rf_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_classifier.fit(X_train_subset_2_7, y_train_subset_2_7)\n",
    "\n",
    "# Feature importance without hyperparameter tuning\n",
    "feature_importance_rf = rf_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame for the feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': X_train_subset_2_7.columns, 'Importance': feature_importance_rf})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "feature_importance['Importance'] = feature_importance['Importance'].round(6)\n",
    "\n",
    "# Print variables with importance >= 0.001\n",
    "print(\"Variables with importance >= 0.001:\")\n",
    "print(feature_importance[feature_importance['Importance'] >= 0.001])\n",
    "\n",
    "# Remove variables with importance < 0.001\n",
    "low_importance_variables = feature_importance[feature_importance['Importance'] < 0.001]['Feature'].tolist()\n",
    "X_train_subset_2_7 = X_train_subset_2_7.drop(columns=low_importance_variables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
